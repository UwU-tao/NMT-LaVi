{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, html, laonlp, underthesea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../raw/train2023.lo', 'r', encoding='utf-8') as f, open('../prepared/train2023.lo', 'w', encoding='utf-8') as f2:\n",
    "  lines = f.readlines()\n",
    "  for line in lines:\n",
    "    line = laonlp.tokenize.word_tokenize(line)\n",
    "    line = [x for x in line if x != ' ']\n",
    "    line = ' '.join(line)\n",
    "    f2.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from underthesea import word_tokenize\n",
    "with open('../raw/train2023.vi', 'r', encoding='utf-8') as f, open('../prepared/train2023.vi', 'w', encoding='utf-8') as f2:\n",
    "  lines = f.readlines()\n",
    "  for line in lines:\n",
    "    line = word_tokenize(line)\n",
    "    line = ' '.join(line)\n",
    "    f2.write(line + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# data_la = np.array(data_la)\n",
    "# data_vi = np.array(data_vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100001 100001\n"
     ]
    }
   ],
   "source": [
    "print(len(data_vi), len(data_la))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in data_la:\n",
    "    data = laonlp.tokenize.word_tokenize(data)\n",
    "    data = [x for x in data if x != ' ']\n",
    "    data = ' '.join(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ຖ້າ ເຈົ້າ ຮູ້ສຶກ ຢ້ານ ທີ່ ຈະ ປະກາດ ຂໍ ໃຫ້ ຊ້ອມ ເວົ້າ ໃນ ແບບ ທີ່ ຍັງ ບໍ່ ມີ ເປົ້າ ຫມາຍ ປະກາດ.',\n",
       " 'ຄົນ ຕໍ່າ ຕ້ອຍ ຈໍານວນ ຫຼາຍ ຄົງ ຈະ ຫຼົບ ຫຼີກ ຜູ້ ນໍາ ດັ່ງ ກ່າວ ແທນ ທີ່ ຈະ ຂໍ ຄວາມ ຊ່ວຍເຫຼືອ ຫຼື ການ ຊີ້ ນໍາ ຈາກ ເຂົາ ເຈົ້າ.',\n",
       " '21 ບັດ ນີ້ເມື່ອລາຊິນີ ເຫັນ ຄວາມ ຢ້ານ ກົວ ຂອງ ພວກ ຂ້າ ໃຊ້ ນາງ ກໍ ເລີ່ມ ມີ ຄວາມ ຢ້ານ ກົວ ຫລາຍ ຂຶ້ນ, ຢ້ານ ວ່າ ສິ່ງ ບໍ່ ດີ ຈະ ເກີດ ກັບ ນາງ.',\n",
       " 'ການ ຕັດສິນ ໃຈ ທີ່ ຈະ ປ່ຽນແປງ ກໍ ເປັນ ຂອງ ທ່ານ , ແລະ ເປັນ ຂອງ ທ່ານ ຄົນ ດຽວ.',\n",
       " 'ການ ເລືອກ ທີ່ ພວກ ເຈົ້າ ເຮັດ ໃນ ເວລາ ນີ້ ມີ ຄວາມ ສໍາຄັນ ຕະຫລອດ ການ.',\n",
       " 'ໂດຍ ທີ່ ຄິດ ແນວ ນີ້, ມີ ຄົນ ໃດ ແດ່ ທີ່ ຕ້ອງການ ກໍາລັງ ໃຈ ຈາກ ທ່ານ?',\n",
       " 'ພວກ ເຂົາ ຢືນ ຢູ່ ຢ່າງທຸ່ນ ທ່ຽງ , “ແນ່ວ ແນ່ ແລະ ບໍ່ ຫວັ່ນ ໄຫວ”2 ຢູ່ ໃນ ສະ ຖາ ນະ ການ ແລະ ສະ ພາບ ແວດ ລ້ອມ ອັນ ທ້າ ທາຍ ທີ່ ແຕກ ຕ່າງ ກັນ.',\n",
       " 'ທ່ານ ໂຊກ ດີ ຫລາຍ ທີ່ ຮູ້ ວ່າ ເຂົາ ເຈົ້າ ໄດ້ ຮຽນ ຮູ້ ແຜນ ແຫ່ງ ຄວາມ ລອດ ຈາກ ການ ສອນ ທີ່ ເຂົາ ເຈົ້າ ໄດ້ ຮັບ ຢູ່ ໃນ ໂລກ ກ່ອນ ເກີດ.',\n",
       " 'ຄົນ ທີ່ ນັ່ງ ຢູ່ ອ້ອມ ຮອບ ທ່ານ ໃນ ກອງ ປະຊຸມ ນີ້ ຕ້ອງການ ທ່ານ.',\n",
       " 'ແລ້ວ ທ່ານ ຈະ ສາມາດ ຊ່ອຍ ຄົນ ອື່ນ ເຮັດ ຄື ກັນ.',\n",
       " 'ທ່ານ ກໍ ຄົງ ມີ ຄວາມ ຢ້ານ ຫລາຍ ເມື່ອ ຮູ້ ວ່າ ຕົນ ເອງ ເປັນ ພະຍາດ, ຫລື ຄົນ ໃນ ຄອບຄົວ ມີ ບັນຫາ ຫລື ຢູ່ ໃນ ໄພ ອັນຕະລາຍ ໃດໆ, ຫລື ຈາກ ການ ສັງເກດ ເບິ່ງ ເຫດການ ທີ່ ເກີດ ຂຶ້ນ ໃນ ໂລກ ທີ່ ກວນ ໃຈ.',\n",
       " 'ພຣະອົງ ຮັກ ທ່ານ ໃນ ເວລາ ນີ້ ດ້ວຍ ຄວາມ ເຂົ້າ ໃຈ ທັງ ຫມົດ ເຖິງ ການ ດີ້ນ ລົນ ຂອງ ທ່ານ.',\n",
       " 'ເຂົາເຈົ້າ ໄດ້ ເອົາໃຈໃສ່ຄໍາ ສອນ ທີ່ ສໍາຄັນ ໂດຍ ການ ສຶກ ສາ ພຣະ ຄໍາ ພີ.',\n",
       " 'ຖ້າ ມີ ຄົນ ຖາມ ລູກ ກ່ຽວ ກັບ ເລື່ອງ ນີ້ ລູກ ຄວນ ຈະ ເວົ້າ ວ່າ ອ້າຍ ເອື້ອຍ ນ້ອງ ເປັນ ຜູ້ ເຮັດ ບໍ?',\n",
       " 'ເມື່ອ ພໍ່ (ແມ່) ເຕືອນ ລູກ ກ່ຽວ ກັບ ຜູ້ ຄົນ ຫຼື ສະຖານ ທີ່ ທີ່ ອາດ ເປັນ ອັນຕະລາຍ ລູກ ຕ້ອງ ເຊື່ອ ຟັງ.',\n",
       " 'ຊາຍ ຄົນ ນີ້ ຄວນ ໃຫ້ ເວລາ ເພື່ອນ ຂ້າ ໃຊ້ ຂອງ ລາວ ຫຼາຍ ກວ່າ ນີ້ ບໍ?— ຖ້າ ເປັນ ລູກ ລູກ ຈະ ເຮັດ ແນວ ໃດ?—',\n",
       " '“ສະນັ້ນ, ພາ ບະ ໂລ, ເພິ່ນ ໄດ້ ບອກ ຫຍັງ ເຈົ້າ ເມື່ອ ເຈົ້າ ມີ ອາຍຸ 10 ປີ?”',\n",
       " 'ແມ່ ຂອງ ນາງ ໄດ້ ຂີ່ ເຮືອ ຕາມ ນາງ ໄປຂ້າງໆ, ແລະ ນາງ ຟະ ລໍ ເຣັນ ໄດ້ ບອກ ແມ່ ຂອງ ນາງ ວ່າ ນາງ ຢ້ານ ວ່າ ນາງ ຄົງ ໄປ ບໍ່ ເຖິງ ຝັ່ງ.',\n",
       " 'ຈະ ວ່າ ແນວ ໃດ ຖ້າ ລູກ ແຂງແຮງ ກວ່າ ນ້ອງ ຊາຍ ຫຼື ນ້ອງ ສາວ ຂອງ ລູກ ເດ?',\n",
       " 'ລູກ ຮູ້ຈັກ ຄົນ ທີ່ ມີ ອາຍຸ ຫນ້ອຍ ກວ່າ ລູກ ບໍ?— ລູກ ຮູ້ ຫຼາຍ ກວ່າ ເຂົາ ເຈົ້າ ບໍ?— ເປັນ ຫຍັງ ລູກ ຈຶ່ງ ຮູ້ ຫຼາຍ ກວ່າ ເຂົາ ເຈົ້າ?— ກໍ ຍ້ອນ ວ່າ ລູກ ເກີດ ກ່ອນ ເຂົາ ເຈົ້າ.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_la[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = [x for x,y in enumerate(data_vi) if len(y.split(' ')) < 10]\n",
    "f2 = [x for x,y in enumerate(data_vi) if len(y.split(' ')) > 150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.delete(data_vi, np.append(f1, f2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = np.delete(data_la, np.append(f1, f2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 't' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\LENOVO\\Documents\\Projects\\NMT-LaVi\\data\\raw\\preprocess.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/Documents/Projects/NMT-LaVi/data/raw/preprocess.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mlen\u001b[39m(t), \u001b[39mlen\u001b[39m(t2))\n",
      "\u001b[1;31mNameError\u001b[0m: name 't' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(t), len(t2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../test/train2023.vi', 'w', encoding='utf-8') as f:\n",
    "  f.write('\\n'.join(data_vi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../test/test.lo', 'w', encoding='utf-8') as f:\n",
    "  f.write('\\n'.join(data_la))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2272\n"
     ]
    }
   ],
   "source": [
    "print(len([x for x in data_vi if len(x) > 500]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text without accents: Mot con meo đang yeu! Co ay thich an ca.\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\LENOVO/nltk_data'\n    - 'd:\\\\miniconda3\\\\envs\\\\hieu\\\\nltk_data'\n    - 'd:\\\\miniconda3\\\\envs\\\\hieu\\\\share\\\\nltk_data'\n    - 'd:\\\\miniconda3\\\\envs\\\\hieu\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\LENOVO\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\LENOVO\\Documents\\Projects\\NMT-LaVi\\data\\raw\\preprocess.ipynb Cell 14\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/Documents/Projects/NMT-LaVi/data/raw/preprocess.ipynb#X16sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mText without accents:\u001b[39m\u001b[39m\"\u001b[39m, cleaned_text)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/Documents/Projects/NMT-LaVi/data/raw/preprocess.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# Tokenize the text\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/Documents/Projects/NMT-LaVi/data/raw/preprocess.ipynb#X16sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m tokens \u001b[39m=\u001b[39m word_tokenize(cleaned_text\u001b[39m.\u001b[39;49mlower())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/LENOVO/Documents/Projects/NMT-LaVi/data/raw/preprocess.ipynb#X16sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTokens:\u001b[39m\u001b[39m\"\u001b[39m, tokens)\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\hieu\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\hieu\\lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\hieu\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\hieu\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[1;32md:\\miniconda3\\envs\\hieu\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\LENOVO/nltk_data'\n    - 'd:\\\\miniconda3\\\\envs\\\\hieu\\\\nltk_data'\n    - 'd:\\\\miniconda3\\\\envs\\\\hieu\\\\share\\\\nltk_data'\n    - 'd:\\\\miniconda3\\\\envs\\\\hieu\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\LENOVO\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import unicodedata\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Sample Vietnamese text\n",
    "vietnamese_text = \"Một con mèo đáng yêu! Cô ấy thích ăn cá.\"\n",
    "\n",
    "# Remove accents\n",
    "def remove_accents(text):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', text)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "cleaned_text = remove_accents(vietnamese_text)\n",
    "print(\"Text without accents:\", cleaned_text)\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(cleaned_text.lower())\n",
    "print(\"Tokens:\", tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hieu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
